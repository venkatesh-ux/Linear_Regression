# -*- coding: utf-8 -*-
"""Jamboree Education - Linear Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wXOH_8j8ltvshfymNHG_WVQjt6LbFcTy
"""

# Importing the libries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!gdown https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv

df = pd.read_csv('Jamboree_Admission.csv')
df.head()

#checking the shape of the data

df.shape

df.info()

df.describe()

df.rename(columns={'Chance of Admit ':'Chance of Admit','LOR ':'LOR'},inplace=True)

# Correlation of the GRE scaore and Chance of Admit to the univercity
sns.scatterplot(df,x='Chance of Admit',y='GRE Score',hue='Research')
plt.xlabel('Chance of Admit')
plt.ylabel('GRE Score')
plt.show()

"""It is clear in the plot that people with **Research** and higher **GRE Score** have higher chance of admit"""

fig = sns.regplot(df,x="GRE Score",y='CGPA')
plt.title("GRE Score vs TOEFL Score")
plt.show()

"""Although there are exceptions, people with higher CGPA usually have higher GRE scores
maybe because they are smart or hard working
"""

fig = sns.scatterplot(x="CGPA", y="LOR", data=df, hue="Research")
plt.title("GRE Score vs CGPA")
plt.show()

"""LORs are not that related with CGPA so it is clear that a persons LOR is not dependent on
that persons academic excellence. Having research experience is usually related with a
good LOR which might be justied by the fact that supervisors have personal interaction
with the students performing research which usually results in good LORs
"""

fig = sns.scatterplot(x="GRE Score", y="LOR", data=df, hue="Research")
plt.title("GRE Score vs CGPA")
plt.show()

"""GRE scores and LORs are also not that related. People with different kinds of LORs have all
kinds of GRE scores
"""

fig = sns.scatterplot(x="CGPA", y="SOP", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

"""CGPA and SOP are not that related because Statement of Purpose is related to academic
performance, but since people with good CGPA tend to be more hard working so they have
good things to say in their SOP which might explain the slight move towards higher CGPA
as along with good SOPs

"""

fig = sns.scatterplot(x="GRE Score", y="SOP", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

"""Similary, GRE Score and CGPA is only slightly related"""

fig = sns.scatterplot(x="TOEFL Score", y="SOP", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

"""Applicants with dierent kinds of SOP have dierent kinds of TOEFL Score. So the quality of
SOP is not always related to the applicants English skills.
"""

#df['Chance of Admit']
df.columns

from pandas.plotting import scatter_matrix

df.columns

scatter_matrix(df[['GRE Score','TOEFL Score','CGPA','Chance of Admit']],figsize=(8,4))
plt.show()

# Cheching for Correlation

numeric_df = df.select_dtypes(include='number')

correlation_matrix = numeric_df.corr()

print(correlation_matrix)

sns.heatmap(correlation_matrix,annot=True)

"""#Data Preprocessing"""

#Duplicate value check

df[df.duplicated()]

#Missing value treatment

df.isnull().sum()

#Outlier treatment

plt.figure(figsize=[4,5])
sns.boxplot(df['GRE Score'])
plt.show()

df.head()

"""#Model building"""

#Standardization
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
#df = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)
#df.head()

y = df['Chance of Admit']
X = df.drop('Chance of Admit',axis=1)
y.shape, X.shape

#Train test split

from sklearn.model_selection import train_test_split

Xtrain_cv, Xtest, ytrain_cv, ytest = train_test_split(X,y,test_size=0.2,random_state=42)
Xtrain, Xval, ytrain, yval = train_test_split(Xtrain_cv,ytrain_cv,test_size=0.25,random_state=42)

scaler = StandardScaler()
Xtrain_scaled = scaler.fit_transform(Xtrain)
Xtest_scaled = scaler.transform(Xval)

Xtest.shape,Xtrain.shape

Xtrain_scaled

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression() # model creation
model.fit(Xtrain_scaled,ytrain) # model training

model.coef_

model.intercept_

model.score(Xtrain_scaled,ytrain) #LinearRegression train score

y_ = model.predict(Xtrain_scaled)
y_

print('Train score:',model.score(Xtrain_scaled,ytrain))

print('Mean square error:',mean_squared_error(y_,ytrain))

"""##**Valisation score**"""

model = LinearRegression()
model.fit(Xval,yval)

model.coef_

model.intercept_

r2_score = model.score(Xval,yval) #the score is very good on the validation data
r2_score

"""**Linear Regression using Statsmodel library**


* Adjusted. R-squared reects the t of the model. R-squared values range
from 0 to 1,where a higher value generally indicates a better t, assuming certain conditions are
met.
* const coecient is your Y-intercept. It means that if both the Interest_Rate and
Unemployment_Rate coecients are zero, then the expected output (i.e., the Y) would
be equal to the const coecient.
* Interest_Rate coecient represents the change in the output Y due to a change of one
unit in the interest rate (everything else held constant)
* Unemployment_Rate coecient represents the change in the output Y due to a change
of one unit in the unemployment rate (everything else held constant)
* std err reects the level of accuracy of the coecients. The lower it is, the higher is the
level of accuracy
* P >|t| is your p-value. A p-value of less than 0.05 is considered to be statistically
signicant
* Condence Interval represents the range in which our coecients are likely to fall (with
a likelihood of 95%)

**Ajd** **R2** **score**
"""

def adj_r2_score(X,y,r2_score):
  return 1-((1-r2_score)*(len(y-1))/len(y-X.shape[1]-1))

adj_r2_score(Xval,yval,r2_score)

"""##Ridge,Lasso"""

from sklearn.linear_model import Ridge,Lasso
from sklearn.pipeline import make_pipeline

lasso_model = Lasso(alpha=0.01)
Ridge_model = Ridge(alpha=0.5)

lasso_model.fit(Xtrain_scaled,ytrain)
Ridge_model.fit(Xtrain_scaled,ytrain)

lasso_pred = lasso_model.predict(Xtrain_scaled)   # Alpha is the regularization strength
Ridge_pred = Ridge_model.predict(Xtrain_scaled)# Alpha is the regularization strength

lasso_model.coef_

lasso_model.intercept_

lasso_model.score(Xtrain_scaled,ytrain) #Lasso train score

print('Lasso score',lasso_model.score(Xtrain_scaled,ytrain))
print('Ridge score',Ridge_model.score(Xtrain_scaled,ytrain)) #Ridge train score

lasso_model.fit(Xval,yval)
Ridge_model.fit(Xval,yval)

print('Lasso score on validation data',lasso_model.score(Xval,yval))
print('Ridge scoreon validation data',Ridge_model.score(Xval,yval))

"""#Testing the assumptions of the linear regression model

**VIF(Variance Ination Factor)**
* â€œ VIF score of an independent variable represents how well the variable is explained by
other independent variables.
*So, the closer the R^2 value to 1, the higher the value of VIF and the higher the
multicollinearity with the particular independent variable.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

df.shape

# Veriable inflation factors (VIF)

X_t = pd.DataFrame(Xtrain, columns=df.columns[:8])

vif = pd.DataFrame()

vif['Features'] = X_t.columns
vif['VIF'] = [variance_inflation_factor(X_t.values, i) for i in range(X_t.shape[1])]
vif['VIF'] = round(vif['VIF'],2)
vif = vif.sort_values(by = 'VIF', ascending=False)
vif

X_t.drop(columns = 'GRE Score', inplace = True)

vif = pd.DataFrame()


vif['Features'] = X_t.columns
vif['VIF'] = [variance_inflation_factor(X_t.values,i) for i in range(X_t.shape[1])]
vif['VIF'] = round(vif['VIF'],2)
vif = vif.sort_values('VIF',ascending=False)
vif

vif['Features'] = X_t.columns
vif['VIF'] = [variance_inflation_factor(X_t,i) for i in range(X_t.shape[1])]
vif['VIF'] = round(vif['VIF'],2)
vif = vif.sort_values('VIF',ascending=False)
vif

Xtest.drop(columns='GRE Score',inplace=True)
Xval.drop(columns='GRE Score',inplace=True)

X_t.head()

#X_t.drop(columns='Serial No.',axis=1,inplace= True)

model.fit(X_t,ytrain)

model.score(X_t,ytrain)

"""It is clear the "Feature" we have droped is valied and it is clearly evident the the score droped from **0.8286710487778687** to **0.7821882410640142**, So it is not ok droping the feature.

#**Model** **performance** **evaluation**
"""

Xtrain.columns

Xval.columns

from sklearn.metrics import mean_squared_error,r2_score

model.fit(Xtrain,ytrain)

model.score(Xtrain,ytrain)

#Droped the feature because serial no is just has no relavance. We can see there is score drop when we drop the "Serial No."

Xtrain_fin = Xtrain.drop(columns = ['Serial No.'] ,axis=1)
Xtrain_fin.head()

model.fit(Xtrain_fin,ytrain)

model.score(Xtrain_fin,ytrain)

Xval.columns

import statsmodels.api as sm

X_sm = sm.add_constant(Xtrain)

model1 = sm.OLS(ytrain,X_sm)
result = model1.fit()

print(result.summary())

"""It is clear that the score on the train data is descent with the score of **~80%**. The model predict with the good score.

Also it is great fit in the validation data with **~82%**
"""

